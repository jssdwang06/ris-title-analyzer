import re
from collections import Counter
import matplotlib.pyplot as plt

def parse_ris_file(file_path):
    """è§£æRISæ–‡ä»¶å¹¶æå–æ ‡é¢˜"""
    titles = []
    current_title = ""
    in_title = False

    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            print(f"æˆåŠŸæ‰“å¼€æ–‡ä»¶: {file_path}")
            for line in file:
                line = line.strip()

                if line.startswith('TI  - '):  # æ ‡é¢˜å¼€å§‹è¡Œ
                    current_title = line[6:].strip()
                    in_title = True
                elif in_title and line and not line.startswith(('TY  -', 'AU  -', 'PY  -', 'T2  -', 'VL  -', 'IS  -', 'SP  -', 'EP  -', 'DO  -', 'UR  -', 'AB  -', 'KW  -', 'M3  -', 'DB  -', 'N1  -', 'ER  -', 'C7  -', 'ST  -')):
                    # æ ‡é¢˜å¯èƒ½è·¨å¤šè¡Œï¼Œç»§ç»­æ·»åŠ 
                    current_title += " " + line
                elif line.startswith('ER  - ') or (in_title and line.startswith(('AU  -', 'PY  -', 'T2  -', 'AB  -'))):  # è®°å½•ç»“æŸæˆ–å…¶ä»–å­—æ®µå¼€å§‹
                    if current_title:
                        # æ¸…ç†æ ‡é¢˜ï¼Œç§»é™¤å¤šä½™ç©ºæ ¼
                        clean_title = ' '.join(current_title.split())
                        titles.append(clean_title)
                    current_title = ""
                    in_title = False

            # å¤„ç†æ–‡ä»¶æœ«å°¾æ²¡æœ‰ERæ ‡è®°çš„æƒ…å†µ
            if current_title:
                clean_title = ' '.join(current_title.split())
                titles.append(clean_title)
                print(f"æ–‡ä»¶æœ«å°¾æ ‡é¢˜: {clean_title}")

            if len(titles) == 0:
                print("è­¦å‘Š: æœªæ‰¾åˆ°ä»»ä½•ä»¥'TI  - 'å¼€å¤´çš„æ ‡é¢˜è¡Œ")
                print("æ–‡ä»¶å‰20è¡Œå†…å®¹é¢„è§ˆ:")
                with open(file_path, 'r', encoding='utf-8') as preview:
                    for i, line in enumerate(preview):
                        if i < 20:
                            print(f"{i+1}: {line.strip()}")
                        else:
                            break
    except UnicodeDecodeError:
        print("UTF-8ç¼–ç æ‰“å¼€å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ...")
        try:
            with open(file_path, 'r', encoding='latin-1') as file:
                print("ä½¿ç”¨latin-1ç¼–ç é‡æ–°è§£æ...")
                for line in file:
                    line = line.strip()

                    if line.startswith('TI  - '):
                        current_title = line[6:].strip()
                        in_title = True
                    elif in_title and line and not line.startswith(('TY  -', 'AU  -', 'PY  -', 'T2  -', 'VL  -', 'IS  -', 'SP  -', 'EP  -', 'DO  -', 'UR  -', 'AB  -', 'KW  -', 'M3  -', 'DB  -', 'N1  -', 'ER  -', 'C7  -', 'ST  -')):
                        current_title += " " + line
                    elif line.startswith('ER  - ') or (in_title and line.startswith(('AU  -', 'PY  -', 'T2  -', 'AB  -'))):
                        if current_title:
                            clean_title = ' '.join(current_title.split())
                            titles.append(clean_title)
                        current_title = ""
                        in_title = False

                if current_title:
                    clean_title = ' '.join(current_title.split())
                    titles.append(clean_title)
        except Exception as e:
            print(f"æ‰“å¼€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    except Exception as e:
        print(f"æ‰“å¼€æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    return titles

def normalize_word(word):
    """è¯å½¢è§„èŒƒåŒ–ï¼šå°†å¤æ•°å½¢å¼è½¬æ¢ä¸ºå•æ•°å½¢å¼"""
    # å¤„ç†å¸¸è§çš„å¤æ•°å½¢å¼
    if word.endswith('ies') and len(word) > 4:
        # studies -> study, theories -> theory
        return word[:-3] + 'y'
    elif word.endswith('es') and len(word) > 3:
        # processes -> process, analyses -> analysis
        if word.endswith('ses'):
            return word[:-2]  # analyses -> analysis
        elif word.endswith('ches') or word.endswith('shes') or word.endswith('xes'):
            return word[:-2]  # approaches -> approach
        else:
            return word[:-1]  # nodes -> node
    elif word.endswith('s') and len(word) > 3:
        # systems -> system, networks -> network
        # ä½†è¦é¿å…è¯¯å¤„ç†ä»¥sç»“å°¾çš„å•æ•°è¯
        if not word.endswith(('ss', 'us', 'is', 'as', 'os')):
            return word[:-1]

    return word

def analyze_word_frequency(titles, top_n=50):
    """åˆ†ææ ‡é¢˜ä¸­çš„è¯é¢‘"""
    print(f"å¼€å§‹åˆ†æ {len(titles)} ä¸ªæ ‡é¢˜çš„è¯é¢‘...")

    # å­¦æœ¯æ–‡ç« æ ‡é¢˜ä¸“ç”¨åœç”¨è¯åˆ—è¡¨
    # ä¿ç•™å­¦æœ¯ä»·å€¼é«˜çš„è¯æ±‡ï¼Œç§»é™¤å¸¸è§çš„åŠŸèƒ½è¯å’Œè¿æ¥è¯
    stop_words = {
        # åŸºç¡€åŠŸèƒ½è¯
        'a', 'an', 'the', 'and', 'or', 'but', 'if', 'of', 'at', 'by',
        'for', 'with', 'about', 'to', 'from', 'in', 'on', 'is', 'are',
        'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had',
        'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may',
        'might', 'must', 'can', 'this', 'that', 'these', 'those',

        # ä»£è¯ï¼ˆåœ¨å­¦æœ¯æ ‡é¢˜ä¸­å¾ˆå°‘å‡ºç°ï¼Œä½†ä¿ç•™ä»¥é˜²ä¸‡ä¸€ï¼‰
        'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her',
        'us', 'them', 'my', 'your', 'his', 'its', 'our', 'their',

        # ç–‘é—®è¯å’Œå‰¯è¯ï¼ˆæŸäº›åœ¨å­¦æœ¯æ ‡é¢˜ä¸­å¯èƒ½æœ‰æ„ä¹‰ï¼Œè°¨æ…ç§»é™¤ï¼‰
        'what', 'which', 'who', 'whom', 'whose', 'where', 'when', 'why', 'how',

        # é‡è¯å’Œé™å®šè¯
        'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such',
        'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very',

        # å¸¸è§å‰¯è¯å’Œä»‹è¯
        'just', 'now', 'also', 'as', 'up', 'out', 'down', 'off', 'over', 'under',
        'again', 'further', 'then', 'once', 'here', 'there', 'into', 'onto',
        'upon', 'within', 'without', 'through', 'during', 'before', 'after',
        'above', 'below', 'between', 'among',

        # å­¦æœ¯æ ‡é¢˜ä¸­å¸¸è§ä½†ä¿¡æ¯é‡è¾ƒä½çš„è¯æ±‡
        'study', 'studies', 'research', 'investigation', 'paper', 'article',
        'review', 'survey', 'overview', 'introduction', 'conclusion',
        'case', 'cases', 'example', 'examples', 'method', 'methods',
        'approach', 'approaches', 'technique', 'techniques', 'way', 'ways',
        'new', 'novel', 'improved', 'enhanced', 'advanced', 'modern',
        'recent', 'current', 'latest', 'state', 'art', 'based', 'using',
        'via', 'through', 'toward', 'towards', 'into', 'onto', 'upon'
    }

    # ä½¿ç”¨ç®€å•çš„ç©ºæ ¼åˆ†è¯
    all_words = []
    total_titles = len(titles)

    # æ˜¾ç¤ºè¿›åº¦ï¼Œæ¯å¤„ç†500ä¸ªæ ‡é¢˜æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
    for i, title in enumerate(titles):
        if (i + 1) % 500 == 0 or i == 0 or i == total_titles - 1:
            print(f"  å¤„ç†è¿›åº¦: {i+1}/{total_titles} ({(i+1)/total_titles*100:.1f}%)")

        # è½¬å°å†™ï¼Œç§»é™¤æ ‡ç‚¹ç¬¦å·ï¼Œä¿ç•™å­—æ¯ã€æ•°å­—å’Œç©ºæ ¼
        clean_title = re.sub(r'[^\w\s]', ' ', title.lower())
        # åˆ†è¯
        words = clean_title.split()
        # è¿‡æ»¤å¹¶è§„èŒƒåŒ–ï¼šåªä¿ç•™å­—æ¯å•è¯ï¼Œé•¿åº¦å¤§äº2ï¼Œä¸åœ¨åœç”¨è¯ä¸­ï¼Œç„¶åè¿›è¡Œè¯å½¢è§„èŒƒåŒ–
        filtered_words = []
        for word in words:
            if word.isalpha() and len(word) > 2 and word not in stop_words:
                normalized_word = normalize_word(word)
                # å†æ¬¡æ£€æŸ¥è§„èŒƒåŒ–åçš„è¯æ˜¯å¦åœ¨åœç”¨è¯ä¸­
                if normalized_word not in stop_words:
                    filtered_words.append(normalized_word)
        all_words.extend(filtered_words)

    print(f"âœ“ æ€»å…±æå–åˆ° {len(all_words)} ä¸ªè¯æ±‡")

    # è®¡ç®—è¯é¢‘
    word_counts = Counter(all_words)
    most_common = word_counts.most_common(top_n)

    print(f"âœ“ æ‰¾åˆ° {len(word_counts)} ä¸ªä¸åŒçš„è¯æ±‡")
    return most_common

def plot_word_frequency(word_counts):
    """ç»˜åˆ¶è¯é¢‘åˆ†å¸ƒå›¾"""
    if len(word_counts) == 0:
        print("æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®æ¥ç”Ÿæˆè¯é¢‘å›¾")
        return

    words, counts = zip(*word_counts)

    # è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
    plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS']
    plt.rcParams['axes.unicode_minus'] = False

    plt.figure(figsize=(15, 8))
    bars = plt.bar(words, counts, color='skyblue', alpha=0.7)

    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ˜¾ç¤ºæ•°å€¼
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                str(count), ha='center', va='bottom', fontsize=10)

    plt.xticks(rotation=45, ha='right', fontsize=12)
    plt.title('æ ‡é¢˜è¯é¢‘åˆ†å¸ƒ (Title Word Frequency)', fontsize=16, fontweight='bold')
    plt.xlabel('è¯è¯­ (Words)', fontsize=14)
    plt.ylabel('é¢‘æ¬¡ (Frequency)', fontsize=14)
    plt.grid(axis='y', alpha=0.3)
    plt.tight_layout()

    # ä¿å­˜å›¾ç‰‡
    plt.savefig('title_word_frequency.png', dpi=300, bbox_inches='tight')
    print("è¯é¢‘å›¾å·²ä¿å­˜ä¸º 'title_word_frequency.png'")
    plt.show()

if __name__ == "__main__":
    file_path = input("è¯·è¾“å…¥RISæ–‡ä»¶è·¯å¾„: ")

    print("=" * 60)
    print("RISæ–‡ä»¶æ ‡é¢˜è§£æå’Œè¯é¢‘åˆ†æå·¥å…·")
    print("=" * 60)

    titles = parse_ris_file(file_path)
    print(f"\nâœ“ æˆåŠŸè§£æå‡º {len(titles)} ä¸ªæ ‡é¢˜")

    if len(titles) == 0:
        print("âŒ æœªæ‰¾åˆ°æ ‡é¢˜ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶è·¯å¾„å’Œæ ¼å¼æ˜¯å¦æ­£ç¡®")
    else:
        # æ˜¾ç¤ºå‰å‡ ä¸ªæ ‡é¢˜ä½œä¸ºç¤ºä¾‹
        print("\nğŸ“‹ æ ‡é¢˜ç¤ºä¾‹:")
        for i, title in enumerate(titles[:5]):
            print(f"  {i+1}. {title}")
        if len(titles) > 5:
            print(f"  ... è¿˜æœ‰ {len(titles) - 5} ä¸ªæ ‡é¢˜")

        print("\n" + "=" * 60)
        print("å¼€å§‹è¯é¢‘åˆ†æ...")
        print("=" * 60)

        word_counts = analyze_word_frequency(titles)

        if word_counts:
            print(f"\nğŸ“Š è¯é¢‘ç»Ÿè®¡ç»“æœ (å‰ {len(word_counts)} ä¸ªé«˜é¢‘è¯):")
            print("-" * 40)
            for i, (word, count) in enumerate(word_counts, 1):
                print(f"{i:2d}. {word:<20} : {count:3d} æ¬¡")

            print("\nğŸ“ˆ æ­£åœ¨ç”Ÿæˆè¯é¢‘åˆ†å¸ƒå›¾...")
            plot_word_frequency(word_counts)
            print("âœ“ åˆ†æå®Œæˆï¼")
        else:
            print("âŒ æœªèƒ½æå–åˆ°æœ‰æ•ˆè¯æ±‡ï¼Œè¯·æ£€æŸ¥æ ‡é¢˜å†…å®¹")

    print("\n" + "=" * 60)
    print("ç¨‹åºæ‰§è¡Œå®Œæ¯•")
    print("=" * 60)

